---
title: "Boostcamp AI Tech OnBoarding Course(Day 3)"
date: 2024-07-31
layout: post
tags: [Naver Boostcamp, daily report]
---

## 1. 실습

* Recurrent Neural Networks
- [x] LSTM(Long short-term memory)을 사용한 MNIST classification 구현
<br><br>

* Transformer
 - [x] SDPA(Scaled Dop-Product Attention) 구현
 - [x] MHA (Multi-Head Attention) 구현
 - [x] Position embedding plot 결과 학습
<br><br>

## 2. 과제 제출

* LSTM Assignment (Assignment4)
 - [x] LSTM(Long short-term memory)을 사용한 MNIST classification을 구현
 - [x] 코드가 정상적으로 실행된다면 일정 epoch마다 그래프가 나온다.
<br><br>

* Multi-head Attention Assignment (Assignment5)
 - [x] Transformer에서 제안된 SDPA (Scaled Dop-Product Attention)과 MHA (Multi-Head Attention)를 구현
 - [x] 모델의 학습이 아닌 논문의 수식을 올바르게 구현하는게 목적
<br><br>
	
## 3. 심화 과제 제출

* ViT (Assignment6)
 - [ ] MHA(Multi-headed Attention)을 computer vision 문제에 적용한 ViT 모델을 직접 구현
 - [ ] 최근 다양한 분야에서 활발하게 적용되고 있는 transformer를 어떤 식으로 다른 문제에 적용할 수 있는지 확인하고, 모델 구조 설명을 기반으로 실제 pytorch 코드로 구현
 - [ ] Training과 testing 모듈, 그리고 attention map을 visualization하는 코드로 직접 구현한 모델이 잘 작동하는지 확인
<br><br>

* AAE (Assignment7)
 - [ ] Generative model인 Adversarial Autoencoder (AAE)의 Encoder와 Discriminator에 필요한 Layer들을 직접 쌓아보는 실습
 - [ ] Decoder의 layer 예시를 보고 layer과 파라미터를 자유롭게 변경
 - [ ] AAE의 개념과 VAE의 Reparametrization 트릭에 대해서 배움
 - [ ] Hyperparameter를 변경면서 자유롭게 네트워크를 train/inference 
<br><br>

## 4. 오늘 들은 강의의 Further Questions

- CNN 모델이 Sequential Data를 처리하는 데에는 어떤 한계점이 있는지
- LSTM 에서는 Modern CNN 내용에서 배웠던 개념 중 어떤 것이 적용되어 있는지
- RNN, LSTM, GRU의 구조적 차이점과 장단점이 무엇인지
- Transformer의 구조에서 Query, Key, Value가 각각 어떤 역할을 하는지
<br><br>

## 4. 오늘 들은 강의의 Further Reading

- [RNN과 LSTM](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
- [The Illustrated Transformer ](https://jalammar.github.io/illustrated-transformer/)
- [Transformer Pytorch official tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
<br><br>
