---
title: "Boostcamp AI Tech (Day 036)"
date: 2024-09-23
layout: post
tags: [Naver Boostcamp, daily report]
---
## 1. 강의

## 2. 피어세션
- 학습 많이 하자
- 이제 실험 좀하자 :
- sh train.sh 실행
- nohup sh train.sh bg에서 실행
- train.py에서 54번째코드 비활성화하고(wandb init) nohup sh train.sh 하자 우선
- wandb.init(project='my-test-project', config=config)
- timm-efficientnet_b0 얘가 레즈넷보다 낫더라 
- cnn은 이미지 사이즈 암거나 받고 트랜스포머는 224224만 받는다?
- 학습용이랑 추론용 데이터로더 증강이 좀 다르대
- df -H 하면 현재 서버 용량 알 수 잇음
- 옵티마이저 ; SDG, Rmsprop, adam, nadam
- https://velog.io/@hyunicecream/4%EA%B0%80%EC%A7%80-%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80%EC%97%90-%EB%94%B0%EB%A5%B8-%EC%84%B1%EB%8A%A5%ED%8F%89%EA%B0%80-%EB%B9%84%EA%B5%90Adam-vs-Nadam-vs-RMSProp-vs-SGD

- 다중 옵티마이저, 혼합 옵티마이저~
- 스케줄러 ; StepLR 일정 에폭마다, ReduceLROnPlateau 성능 개선 안되면 감소 val loss기반
- 학습률 0.001부터 학습안된대
- AdaGrad 적응적 기울기
  이닛하는과정에서 모델의 파라미터를 씀
    옵티마이저의 가중치는 cpu에 남아잇고
    모델은 쿠다에 가잇으니
    [Effect of calling model.cuda() after constructing an optimizer - PyTorch Forums](https://discuss.pytorch.org/t/effect-of-calling-model-cuda-after-constructing-an-optimizer/15165)
    train.py에서 쿠다로 모델 넣는 코드 위로 올려서 해결~!



## 3. 회고
- 실험은 재미있 재미있?
- 서버 터질까 두렵지않다
- 아 피곤해... 지루해...